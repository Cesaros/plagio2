{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql.types import *\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 1.810s.\n"
     ]
    }
   ],
   "source": [
    "# only run once to transform pdf files to text file\n",
    "import textract\n",
    "import glob\n",
    "from time import time\n",
    "\n",
    "t0=time()\n",
    "path=\"data/\"\n",
    "files=glob.glob(path +'*.pdf')\n",
    "\n",
    "for file in files:\n",
    "    #text_doc = convert(file)\n",
    "    text_doc = textract.process(file).decode('utf-8')\n",
    "    new_path = file.split('.')[0]+\".txt\"\n",
    "    new_file = open(new_path,'w')\n",
    "    new_file.write(text_doc)\n",
    "    new_file.close()\n",
    "\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "# to apply directly in the command line\n",
    "shopt -s nullglob\n",
    "for f in *.txt\n",
    "do\n",
    "    echo \"splitting - $f\"\n",
    "    split -l 500 \"$f\" \"${f%.txt}_\"\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "from time import time\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType\n",
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "from pyspark.sql.functions import explode, col\n",
    "from pyspark.sql.functions import monotonically_increasing_id \n",
    "\n",
    "\n",
    "\n",
    "# Creates a dataframe with the text files\n",
    "def create_dataframe(path_directory=\"data/\", file_extension=\"*.txt\"):    \n",
    "    \n",
    "    path=path_directory\n",
    "    files=glob.glob(path + file_extension)\n",
    "    #print(files)\n",
    "    schema = StructType([StructField(\"row_id\", IntegerType(), True), \n",
    "                         StructField(\"name\", StringType(), True),\n",
    "                     StructField(\"text\", StringType(), True)])\n",
    "    \n",
    "    for idx,f in enumerate(files):\n",
    "        if idx == 0:\n",
    "            with open(f, 'r') as file:\n",
    "                doc= \" \".join(line.strip() for line in file)\n",
    "                df = spark.createDataFrame([(idx, f, doc)],schema)\n",
    "                dff = df\n",
    "        else:\n",
    "            with open(f, 'r') as file:\n",
    "                doc= \" \".join(line.strip() for line in file)\n",
    "                df = spark.createDataFrame([(idx, f, doc)],schema)\n",
    "                dff = dff.union(df)\n",
    "\n",
    "    return dff\n",
    "\n",
    "\n",
    "# Tokenize and remove stopwords from a dataframe\n",
    "\n",
    "def tokenize_dataframe(dataframe, language=\"spanish\", pattern=\"[.:\\s]+\"):    \n",
    "\n",
    "    STOPWORDS_v0 = StopWordsRemover.loadDefaultStopWords(language) \n",
    "    STOPWORDS_v0 = [str(i) for i in STOPWORDS_v0]\n",
    "\n",
    "    tokenizer = RegexTokenizer(pattern=pattern, inputCol='text', outputCol=\"z_words\")\n",
    "    wordsData1 = tokenizer.transform(dataframe)\n",
    "\n",
    "    remover = StopWordsRemover(inputCol=\"z_words\", outputCol=\"z_filtered\", stopWords=STOPWORDS_v0)\n",
    "    #wordsDataFiltered1 = remover.transform(wordsData1).drop(\"text\",\"z_words\")\n",
    "    wordsDataFiltered1 = remover.transform(wordsData1).drop(\"z_words\")\n",
    "\n",
    "    return wordsDataFiltered1.drop(\"text\")\n",
    "\n",
    "# Split texts in chunks of words, create index\n",
    "\n",
    "def split_chunks(tok_dataframe,n_words=100, inputCol=\"z_filtered\", outputCol=\"list_parag\"):\n",
    "    \n",
    "    def split_list_words(list_words,n_words=n_words):\n",
    "        counter=0\n",
    "        new_list_word=[]\n",
    "        for i in range(0,len(list_words),n_words):\n",
    "                new_list_word.append(list_words[counter*n_words:i+n_words])\n",
    "                counter+=1\n",
    "        return new_list_word\n",
    "\n",
    "    dummy_function_udf = udf(split_list_words, ArrayType(ArrayType(StringType())))\n",
    "    wordsDataFiltered2=tok_dataframe.select(\"row_id\",inputCol, \n",
    "                              dummy_function_udf(inputCol).alias(outputCol))\n",
    "    \n",
    "    wordsDataFiltered3 = wordsDataFiltered2.select(\"row_id\",explode(col(outputCol)))\n",
    "    df_index = wordsDataFiltered3.withColumn(\"para_id\", monotonically_increasing_id())\n",
    "\n",
    "    return df_index\n",
    "\n",
    "\n",
    "#Create tf\n",
    "\n",
    "def create_TF(dataframe, inputCol=\"z_filtered\", outputCol=\"features\", minDocFreq=3, \n",
    "              numFeatures=20):    \n",
    "\n",
    "    hashingTF = HashingTF(inputCol=inputCol, outputCol=outputCol, numFeatures=numFeatures)\n",
    "    featurizedData1 = hashingTF.transform(dataframe)\n",
    "\n",
    "    return featurizedData1.drop(inputCol)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 5.425s.\n",
      "+------+--------------------+--------------------+\n",
      "|row_id|                name|                text|\n",
      "+------+--------------------+--------------------+\n",
      "|     0|data_split/AREQUI...|PONTIFICIA UNIVER...|\n",
      "|     1|data_split/AREQUI...| Iniciativas soci...|\n",
      "|     2|data_split/AREQUI...|medio ambiente y ...|\n",
      "|     3|data_split/AREQUI...|un comportamiento...|\n",
      "|     4|data_split/AREQUI...|investigación: 1....|\n",
      "|     5|data_split/AREQUI...|Sí  12  Sí  Sí  S...|\n",
      "|     6|data_split/AREQUI...|t  i  bi S i ...|\n",
      "|     7|data_split/AREQUI...|fue utilizado par...|\n",
      "|     8|data_split/AREQUI...|image and reputat...|\n",
      "|     9|data_split/AREQUI...|Sólo voltear la p...|\n",
      "|    10|data_split/AREQUI...| Apoya a programa...|\n",
      "|    11|data_split/CABRER...|PONTIFICIA UNIVER...|\n",
      "|    12|data_split/CABRER...|infraestructura, ...|\n",
      "|    13|data_split/CABRER...|también un destin...|\n",
      "|    14|data_split/CABRER...| La desmotivación...|\n",
      "|    15|data_split/CABRER...| 0.90  Valo r 4  ...|\n",
      "|    16|data_split/CABRER...| 0.10  3  0.15  3...|\n",
      "|    17|data_split/CABRER...|tendencia a la re...|\n",
      "|    18|data_split/CABRER...|Atlántico puede c...|\n",
      "|    19|data_split/CABRER...|4 Rango de precio...|\n",
      "+------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t0=time()\n",
    "dff=create_dataframe(path_directory=\"data_split/\", file_extension='*')   \n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "dff.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "177"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dff.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.165s.\n",
      "+------+--------------------+--------------------+\n",
      "|row_id|                name|          z_filtered|\n",
      "+------+--------------------+--------------------+\n",
      "|     0|data_split/AREQUI...|[pontificia, univ...|\n",
      "|     1|data_split/AREQUI...|[iniciativas, soc...|\n",
      "|     2|data_split/AREQUI...|[medio, ambiente,...|\n",
      "|     3|data_split/AREQUI...|[comportamiento, ...|\n",
      "|     4|data_split/AREQUI...|[investigación, 1...|\n",
      "|     5|data_split/AREQUI...|[12, x, 10, 12, x...|\n",
      "|     6|data_split/AREQUI...|[t, i, , bi, s...|\n",
      "|     7|data_split/AREQUI...|[utilizado, calcu...|\n",
      "|     8|data_split/AREQUI...|[image, and, repu...|\n",
      "|     9|data_split/AREQUI...|[sólo, voltear, p...|\n",
      "|    10|data_split/AREQUI...|[apoya, programas...|\n",
      "|    11|data_split/CABRER...|[pontificia, univ...|\n",
      "|    12|data_split/CABRER...|[infraestructura,...|\n",
      "|    13|data_split/CABRER...|[destino, importa...|\n",
      "|    14|data_split/CABRER...|[desmotivación, c...|\n",
      "|    15|data_split/CABRER...|[0, 90, valo, r, ...|\n",
      "|    16|data_split/CABRER...|[0, 10, 3, 0, 15,...|\n",
      "|    17|data_split/CABRER...|[tendencia, reduc...|\n",
      "|    18|data_split/CABRER...|[atlántico, puede...|\n",
      "|    19|data_split/CABRER...|[4, rango, precio...|\n",
      "+------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tokenize and remove stopwords\n",
    "\n",
    "\n",
    "t0=time()\n",
    "wordsDataFiltered1 = tokenize_dataframe(dff, language=\"spanish\", pattern=\"[.:\\s]+\")\n",
    "#wordsDataFiltered2 = split_chunks(wordsDataFiltered1,n_words=100, inputCol=\"z_filtered\", \n",
    "                                  #outputCol=\"list_parag\")\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "wordsDataFiltered1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+--------------------+\n",
      "|row_id|                name|            features|\n",
      "+------+--------------------+--------------------+\n",
      "|     0|data_split/AREQUI...|(40000,[20,43,70,...|\n",
      "|     1|data_split/AREQUI...|(40000,[5,7,16,18...|\n",
      "|     2|data_split/AREQUI...|(40000,[7,42,46,5...|\n",
      "|     3|data_split/AREQUI...|(40000,[68,98,104...|\n",
      "|     4|data_split/AREQUI...|(40000,[78,138,22...|\n",
      "+------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "done in 1.973s.\n"
     ]
    }
   ],
   "source": [
    "t0=time()\n",
    "tf_df = create_TF(wordsDataFiltered1, inputCol=\"z_filtered\", minDocFreq=5, numFeatures=40000)\n",
    "tf_df.show(5)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The hashed dataset where hashed values are stored in the column 'hashes':\n",
      "+------+--------------------+--------------------+--------------------+\n",
      "|row_id|                name|            features|              hashes|\n",
      "+------+--------------------+--------------------+--------------------+\n",
      "|     0|data_split/AREQUI...|(40000,[20,43,70,...|[[-2.034555E9], [...|\n",
      "|     1|data_split/AREQUI...|(40000,[5,7,16,18...|[[-2.033150372E9]...|\n",
      "|     2|data_split/AREQUI...|(40000,[7,42,46,5...|[[-2.03531092E9],...|\n",
      "|     3|data_split/AREQUI...|(40000,[68,98,104...|[[-2.037395876E9]...|\n",
      "|     4|data_split/AREQUI...|(40000,[78,138,22...|[[-2.029477984E9]...|\n",
      "|     5|data_split/AREQUI...|(40000,[138,192,2...|[[-2.019928688E9]...|\n",
      "|     6|data_split/AREQUI...|(40000,[55,63,104...|[[-2.029477984E9]...|\n",
      "|     7|data_split/AREQUI...|(40000,[20,25,42,...|[[-2.034781776E9]...|\n",
      "|     8|data_split/AREQUI...|(40000,[7,18,20,3...|[[-2.034781776E9]...|\n",
      "|     9|data_split/AREQUI...|(40000,[739,1139,...|[[-1.94047902E9],...|\n",
      "|    10|data_split/AREQUI...|(40000,[124,181,7...|[[-1.969082936E9]...|\n",
      "|    11|data_split/CABRER...|(40000,[39,151,22...|[[-2.034781776E9]...|\n",
      "|    12|data_split/CABRER...|(40000,[20,39,69,...|[[-2.027015068E9]...|\n",
      "|    13|data_split/CABRER...|(40000,[39,55,63,...|[[-2.027015068E9]...|\n",
      "|    14|data_split/CABRER...|(40000,[89,166,20...|[[-2.035235328E9]...|\n",
      "|    15|data_split/CABRER...|(40000,[204,339,7...|[[-2.019928688E9]...|\n",
      "|    16|data_split/CABRER...|(40000,[39,49,69,...|[[-2.035386512E9]...|\n",
      "|    17|data_split/CABRER...|(40000,[21,39,55,...|[[-2.037093508E9]...|\n",
      "|    18|data_split/CABRER...|(40000,[21,39,137...|[[-2.031065416E9]...|\n",
      "|    19|data_split/CABRER...|(40000,[39,55,204...|[[-2.031065416E9]...|\n",
      "+------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "done in 7.086s.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import MinHashLSH, MinHashLSHModel\n",
    "\n",
    "t0=time()\n",
    "\n",
    "mh = MinHashLSH(inputCol=\"features\", outputCol=\"hashes\", numHashTables=5, seed=12345)\n",
    "model = mh.fit(tf_df)\n",
    "\n",
    "# Feature Transformation\n",
    "print(\"The hashed dataset where hashed values are stored in the column 'hashes':\")\n",
    "model.transform(tf_df).show()\n",
    "print(\"done in %0.3fs.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.387s.\n"
     ]
    }
   ],
   "source": [
    "#To correct: Create tf with same words as database of document\n",
    "\n",
    "# Creates a dataframe with the text files for Target_data\n",
    "\n",
    "t0=time()\n",
    "dff_target=create_dataframe(path_directory=\"target_data/\", file_extension=\"*\")  \n",
    "tok_target = tokenize_dataframe(dff_target, language=\"spanish\", pattern=\"[.:\\s]+\")\n",
    "split_in_chunks = split_chunks(tok_target,n_words=100, inputCol=\"z_filtered\", \n",
    "                                  outputCol=\"list_parag\")\n",
    "tf_df_target = create_TF(split_in_chunks, inputCol=\"col\", minDocFreq=5, numFeatures=40000)\n",
    "\n",
    "print(\"done in %0.3fs.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------+\n",
      "|            datasetA|            datasetB|distCol|\n",
      "+--------------------+--------------------+-------+\n",
      "|[173,data_split/V...|[173,data_split/V...|    0.0|\n",
      "|[17,data_split/CA...|[17,data_split/CA...|    0.0|\n",
      "|[139,data_split/O...|[139,data_split/O...|    0.0|\n",
      "|[105,data_split/C...|[105,data_split/C...|    0.0|\n",
      "|[135,data_split/O...|[135,data_split/O...|    0.0|\n",
      "|[137,data_split/O...|[137,data_split/O...|    0.0|\n",
      "|[33,data_split/CA...|[33,data_split/CA...|    0.0|\n",
      "|[0,data_split/ARE...|[0,data_split/ARE...|    0.0|\n",
      "|[34,data_split/CA...|[34,data_split/CA...|    0.0|\n",
      "|[160,data_split/R...|[160,data_split/R...|    0.0|\n",
      "|[128,data_split/O...|[128,data_split/O...|    0.0|\n",
      "|[176,data_split/V...|[176,data_split/V...|    0.0|\n",
      "|[154,data_split/R...|[154,data_split/R...|    0.0|\n",
      "|[118,data_split/G...|[118,data_split/G...|    0.0|\n",
      "|[161,data_split/R...|[161,data_split/R...|    0.0|\n",
      "|[71,data_split/CA...|[71,data_split/CA...|    0.0|\n",
      "|[124,data_split/O...|[124,data_split/O...|    0.0|\n",
      "|[136,data_split/O...|[136,data_split/O...|    0.0|\n",
      "|[62,data_split/CA...|[62,data_split/CA...|    0.0|\n",
      "|[140,data_split/O...|[140,data_split/O...|    0.0|\n",
      "+--------------------+--------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n",
      "done in 48.461s.\n"
     ]
    }
   ],
   "source": [
    "# the lowest the value, the highest the similarity\n",
    "# we filter value = 0 because it is comparing the same documents (, not applicable allways)\n",
    "t0=time()\n",
    "threshold = 0.8\n",
    "#similar=model.approxSimilarityJoin(tf_df, tf_df, threshold).filter(\"distCol != 0\")\n",
    "similar=model.approxSimilarityJoin(tf_df, tf_df, threshold)\n",
    "similar.orderBy(similar.distCol.asc()).show()\n",
    "print(\"done in %0.3fs.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "513"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------------------+\n",
      "|            datasetA|            datasetB|            distCol|\n",
      "+--------------------+--------------------+-------------------+\n",
      "|[22,data_split/CA...|[23,data_split/CA...|0.15000000000000002|\n",
      "|[23,data_split/CA...|[22,data_split/CA...|0.15000000000000002|\n",
      "|[22,data_split/CA...|[64,data_split/CA...|               0.28|\n",
      "|[64,data_split/CA...|[22,data_split/CA...|               0.28|\n",
      "|[64,data_split/CA...|[23,data_split/CA...|0.31999999999999995|\n",
      "|[23,data_split/CA...|[64,data_split/CA...|0.31999999999999995|\n",
      "|[65,data_split/CA...|[66,data_split/CA...| 0.4571428571428572|\n",
      "|[66,data_split/CA...|[65,data_split/CA...| 0.4571428571428572|\n",
      "|[64,data_split/CA...|[65,data_split/CA...| 0.4666666666666667|\n",
      "|[65,data_split/CA...|[64,data_split/CA...| 0.4666666666666667|\n",
      "|[25,data_split/CA...|[20,data_split/CA...| 0.4922779922779923|\n",
      "|[20,data_split/CA...|[25,data_split/CA...| 0.4922779922779923|\n",
      "|[23,data_split/CA...|[65,data_split/CA...| 0.5185185185185186|\n",
      "|[65,data_split/CA...|[23,data_split/CA...| 0.5185185185185186|\n",
      "|[65,data_split/CA...|[22,data_split/CA...| 0.5357142857142857|\n",
      "|[22,data_split/CA...|[65,data_split/CA...| 0.5357142857142857|\n",
      "|[68,data_split/CA...|[62,data_split/CA...| 0.5567010309278351|\n",
      "|[62,data_split/CA...|[68,data_split/CA...| 0.5567010309278351|\n",
      "|[64,data_split/CA...|[66,data_split/CA...| 0.5641025641025641|\n",
      "|[66,data_split/CA...|[64,data_split/CA...| 0.5641025641025641|\n",
      "+--------------------+--------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "similar.filter(\"distCol != 0\").orderBy(similar.distCol.asc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
